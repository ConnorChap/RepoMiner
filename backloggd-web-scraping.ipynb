{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19023dbf-764f-4f36-89d8-a3cdaaeee695",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In order to perform sentiment analysis on user reviews for the selected code repositories, a CSV file of the user reviews and their corresponding game title is required. The web scrape to produce this will be coming from specific urls from https://backloggd.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5922b2-f4d8-4692-aa9b-6d21d2827903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # library used for html web scraping!\n",
    "import soupsieve as sv # library used especially for accessing specific elements in retrieved html responses\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# the following Selenium imports are used before creating soup objects since things like the review body texts are not loaded in with the html \n",
    "# but after (with javascript)\n",
    "# these allow us to access the full page source with beautiful soup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0daf5b4d-6399-4152-beeb-8ba075a6659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining constants / urls\n",
    "BASE_URL = 'https://www.backloggd.com/games/'\n",
    "\n",
    "GAME_URLS = ['daggerfall-unity/', 'luanti/', 'openra/', 'supertux/', 'battle-for-wesnoth/', 'mindustry/', 'shattered-pixel-dungeon/', \n",
    "             'triplea/', 'openttd/', 'nethack/']\n",
    "\n",
    "REVIEW_URL = 'reviews/'\n",
    "\n",
    "num_games = len(GAME_URLS)\n",
    "\n",
    "# create pandas data frame to hold the stuff\n",
    "cols = ['Title', 'Review']\n",
    "df_games = pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d694d07-e04b-4ff5-b414-003eddef3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium set up, NOTE - this cell was generated with the assistance of google gemini:\n",
    "\n",
    "# Setup: Configure Chrome to run without a GUI (headless)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(options=options) \n",
    "# Note: Adjust the path/initialization if you are not using Chrome or need to specify a driver path\n",
    "\n",
    "def fetch_page_with_selenium(url):\n",
    "    \"\"\"Scrolls down repeatedly until the page height stops increasing, \n",
    "    indicating all content (reviews) has been loaded.\"\"\"\n",
    "    \n",
    "    # 1. Navigate to the URL\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Give the initial content time to load\n",
    "    time.sleep(5) \n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        # 2. Scroll to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        # 3. Wait for new content to load (adjust this time if needed)\n",
    "        time.sleep(5) \n",
    "        \n",
    "        # 4. Get the new scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # 5. Check if we've hit the end\n",
    "        if new_height == last_height:\n",
    "            # If the height hasn't changed, we've loaded everything.\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "        \n",
    "    # Now the page source should contain all 30 reviews\n",
    "    return driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed78f519-0596-4ba9-a740-15f4d8dbb59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing all sites succeeded.\n"
     ]
    }
   ],
   "source": [
    "# test html access\n",
    "success = True\n",
    "for game in GAME_URLS:\n",
    "    url = BASE_URL + game + REVIEW_URL\n",
    "    response = requests.get(url)\n",
    "    if (response.status_code != 200):\n",
    "        print(\"error accessing site:\", url)\n",
    "        success = False\n",
    "\n",
    "if(success):\n",
    "    print(\"Accessing all sites succeeded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8bd9d6-43cf-4e4f-8f6b-d9df9e16b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this basically isolates specific html tags, and then gets their text\n",
    "# when isolating css tags, they aren't guaranteed to be \"filled\" this why I check for \"none\"  in order to avoid any attribute errors\n",
    "# note: the following css selectors were chosen from identifying where the texts we need live in the site's html hierarchy (site inspect)\n",
    "\n",
    "def scrape_game_data(page_source):\n",
    "    # create a soup object\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    \n",
    "    title = sv.select_one('p.reviews-game-title a', soup).text\n",
    "   \n",
    "    review_elements = sv.select('.review-body .card-text', soup)\n",
    "    \n",
    "    all_reviews = []\n",
    "    \n",
    "    if review_elements:\n",
    "        for element in review_elements:\n",
    "            all_reviews.append(element.text.strip())\n",
    "    else:\n",
    "        all_reviews = [\"No reviews found\"]\n",
    "    \n",
    "    return [title, all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0417ffc5-7e4c-454d-981b-c13401eb781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading data frame object with data.\n"
     ]
    }
   ],
   "source": [
    "# start collecting info from each game's backloggd review page\n",
    "\n",
    "for game in GAME_URLS:\n",
    "    url = BASE_URL + game + REVIEW_URL\n",
    " \n",
    "    page_source = fetch_page_with_selenium(url)\n",
    "    \n",
    "    title, all_reviews = scrape_game_data(page_source)\n",
    "\n",
    "    # add to data frame in a nice way (one review per row)\n",
    "    for review in all_reviews:\n",
    "        new_row = [title, review]\n",
    "        df_games.loc[len(df_games)] = new_row\n",
    "\n",
    "# close selenium driver to prevent cpu slowdown and memory leakage\n",
    "driver.quit()\n",
    " \n",
    "print(\"Finished loading data frame object with data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160b2ffb-28f9-44e7-a03d-b06f8f7c9fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Done! :3\n"
     ]
    }
   ],
   "source": [
    "# convert the data frame to a csv\n",
    "df_games.to_csv(\"output_data/backloggd_game_reviews.csv\", index = False)\n",
    "print(\"All Done! :3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS480A7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
